{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_model_evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwkhan/CE888/blob/main/tweetAssignment/baseline_model_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNS4t0r_KCud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d6642c-91df-4da1-d90d-6e70b7d96153"
      },
      "source": [
        "!pip install zeugma"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting zeugma\n",
            "  Downloading https://files.pythonhosted.org/packages/59/38/8f57f83719027e36a61238abe1cafa55d257eaaf8e9185b2adbb5a928308/zeugma-0.48.tar.gz\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from zeugma) (1.19.5)\n",
            "Requirement already satisfied: Cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from zeugma) (0.29.21)\n",
            "Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from zeugma) (1.1.5)\n",
            "Requirement already satisfied: gensim>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from zeugma) (3.6.0)\n",
            "Requirement already satisfied: scikit_learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from zeugma) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorflow>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from zeugma) (2.4.1)\n",
            "Requirement already satisfied: keras>=2.1.3 in /usr/local/lib/python3.7/dist-packages (from zeugma) (2.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->zeugma) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->zeugma) (2.8.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.5.0->zeugma) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.5.0->zeugma) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.5.0->zeugma) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>=0.19.1->zeugma) (1.0.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (1.32.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (0.3.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (3.7.4.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (3.12.4)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (1.12.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (2.10.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (2.4.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (1.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (1.6.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.1.3->zeugma) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=1.5.0->zeugma) (53.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (1.27.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (0.4.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (2.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (4.7.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (3.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (3.1.0)\n",
            "Building wheels for collected packages: zeugma\n",
            "  Building wheel for zeugma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zeugma: filename=zeugma-0.48-cp37-none-any.whl size=8778 sha256=0c3db20acda7be7e6ee4857fda415ee1b7d3a3a662ba7631c7c1eda709266753\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/b5/bc/5183ac478b0071d04d3ed0c0dd4a43db94c5c8ffb317b5eb53\n",
            "Successfully built zeugma\n",
            "Installing collected packages: zeugma\n",
            "Successfully installed zeugma-0.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2snf_JzoqiLq",
        "outputId": "ae1c437e-eec4-4d08-afe8-88fbe467b0cb"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk4JkCKPoEwY"
      },
      "source": [
        "# import torch\r\n",
        "# if torch.cuda.is_available():       \r\n",
        "#     device = torch.device(\"cuda\")\r\n",
        "#     print(f'There are {torch.cuda.device_count()} GPU(s) available.')\r\n",
        "#     print('Device name:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "# else:\r\n",
        "#     print('No GPU available, using the CPU instead.')\r\n",
        "#     device = torch.device(\"cpu\")\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPFoHm-Td2zu"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "# Marry nltk and ScikitLearn\r\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\r\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\r\n",
        "import pickle\r\n",
        "from nltk import word_tokenize\r\n",
        "import nltk\r\n",
        "import random\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.classify import ClassifierI\r\n",
        "from statistics import mode\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import seaborn as sns\r\n",
        "import requests\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjorzjloqRO8"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA-D50_uaK7f"
      },
      "source": [
        "############################ SENTIMENT ANALYSIS #################################################\r\n",
        "SENTIMENT_TRAIN_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_text.txt'\r\n",
        "SENTIMENT_VALIDATION_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_text.txt'\r\n",
        "SENTIMENT_TEST_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/test_text.txt'\r\n",
        "\r\n",
        "SENTIMENT_TRAIN_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_labels.txt'\r\n",
        "SENTIMENT_VALIDATION_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_labels.txt'\r\n",
        "SENTIMENT_TEST_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/test_labels.txt'\r\n",
        "\r\n",
        "############################ HATE #################################################\r\n",
        "HATE_TRAIN_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_text.txt'\r\n",
        "HATE_VALIDATION_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_text.txt'\r\n",
        "HATE_TEST_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_text.txt'\r\n",
        "\r\n",
        "HATE_TRAIN_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_labels.txt'\r\n",
        "HATE_VALIDATION_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_labels.txt'\r\n",
        "HATE_TEST_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_labels.txt'\r\n",
        "\r\n",
        "############################ OFFENSIVE LANGUAGE#################################################\r\n",
        "OFFENSE_TRAIN_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_text.txt'\r\n",
        "OFFENSE_VALIDATION_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_text.txt'\r\n",
        "OFFENSE_TEST_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_text.txt'\r\n",
        "\r\n",
        "OFFENSE_TRAIN_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_labels.txt'\r\n",
        "OFFENSE_VALIDATION_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_labels.txt'\r\n",
        "OFFENSE_TEST_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_labels.txt'\r\n",
        "\r\n",
        "############################ IRONY #################################################\r\n",
        "IRONY_TRAIN_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/train_text.txt'\r\n",
        "IRONY_VALIDATION_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/val_text.txt'\r\n",
        "IRONY_TEST_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/test_text.txt'\r\n",
        "\r\n",
        "IRONY_TRAIN_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/train_labels.txt'\r\n",
        "IRONY_VALIDATION_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/val_labels.txt'\r\n",
        "IRONY_TEST_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/test_labels.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5_F0_kzZz14"
      },
      "source": [
        "def preprocess(df): \n",
        "    lemmatizer  = WordNetLemmatizer()\n",
        "    \n",
        "    ps = PorterStemmer()\n",
        "    ignore_words = ['user', 'st'] \n",
        "    df['processed_tweets'] = df['tweet'].replace('[^a-zA-Z]',' ', regex=True,\n",
        "                                                  inplace=False)\n",
        "    df['processed_tweets'] = df['processed_tweets'].apply(lambda x: [w.lower() for w in x.split()])\n",
        "    # df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ([word for word in tweet if not word in stopwords.words(\"english\")]))\n",
        "    # df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ([lemmatizer.lemmatize(word) for word in tweet]))\n",
        "    # df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ([ps.stem(word) for word in tweet]))\n",
        "\n",
        " \n",
        " \n",
        "    df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ' '.join([word for word in tweet if len(word)>2]))\n",
        "    # df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ' '.join([word for word in tweet]))\n",
        "\n",
        " \n",
        "    df['processed_tweets'] = df['processed_tweets'].apply(lambda x: ' '.join([word for word in x.split() if not word in ignore_words]))\n",
        "    \n",
        "    df[\"sentence_length\"] = df.tweet.apply(lambda x: len(str(x).split()))\n",
        "    return df\n",
        " \n",
        " \n",
        "# Wrapper to convert text data to pandas Dataframe\n",
        "def txt_to_df(data, label, classification_task):\n",
        "    tweet = []\n",
        "    sentiments = []\n",
        "    for sentence in data.split('\\n'):\n",
        "        tweet.append(sentence)\n",
        "    for sentiment in label.split('\\n'):\n",
        "        try:\n",
        "            sentiments.append(int(sentiment))\n",
        "        except ValueError:\n",
        "            pass\n",
        "    df= pd.DataFrame(tweet[:-1], columns=['tweet'])\n",
        "    df['label'] = sentiments\n",
        "    if classification_task == 'Sentiment_analysis':\n",
        "      df['sentiment'] = df.label.apply(lambda x: 'Negative'if x==0 else ('Neutral' if x==1 else 'Positive'))\n",
        "    if classification_task == 'hate_analysis':\n",
        "      df['sentiment'] = df.label.apply(lambda x: 'Not-hate'if x==0 else 'hate')\n",
        "    if classification_task == 'offensive_analysis':\n",
        "      df['sentiment'] = df.label.apply(lambda x: 'Not-offensive 'if x==0 else 'offensive')\n",
        "    if classification_task == 'irony_analysis':\n",
        "      df['sentiment'] = df.label.apply(lambda x: 'Non-irony 'if x==0 else 'irony')      \n",
        "    return df\n",
        " \n",
        " \n",
        "def prepare_dataset(TRAIN_TEXT, TRAIN_LABEL, VAL_TEXT, VAL_LABEL, TEST_TEXT, TEST_LABEL, classification_task):\n",
        "  # Reading Train, Vvalidation & Test data from tweeteval Github Repo.\n",
        "  train_tweets_txt = requests.get(TRAIN_TEXT).text\n",
        "  train_labels_txt = requests.get(TRAIN_LABEL).text\n",
        " \n",
        "  val_tweets_txt = requests.get(VAL_TEXT).text\n",
        "  val_labels_txt = requests.get(VAL_LABEL).text\n",
        " \n",
        "  test_tweets_txt = requests.get(TEST_TEXT).text\n",
        "  test_labels_txt = requests.get(TEST_LABEL).text\n",
        " \n",
        "  # Converting text data to pandas Dataframe\n",
        "  train_df = txt_to_df(train_tweets_txt, train_labels_txt, classification_task)\n",
        "  val_df = txt_to_df(val_tweets_txt, val_labels_txt, classification_task)\n",
        "  test_df = txt_to_df(test_tweets_txt, test_labels_txt, classification_task)\n",
        " \n",
        "  train_df = preprocess(train_df)\n",
        "  val_df = preprocess(val_df)\n",
        "  test_df = preprocess(test_df)  \n",
        " \n",
        "  return train_df, val_df, test_df"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TA2OA-k5_ye"
      },
      "source": [
        "#Random OverSampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xzJ-qQ15-Ii"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\r\n",
        "\r\n",
        "def smot_oversmapling(X, y):\r\n",
        "  X = X.reshape(-1,1)\r\n",
        "  oversample = RandomOverSampler()\r\n",
        "\r\n",
        "  X_train_smot, y_train_smot = oversample.fit_resample(X, y)\r\n",
        "  X_train_smot = X_train_smot.reshape(len(X_train_smot),)\r\n",
        "  # print('Shape of Training and Labelled data after random oversampling:', X_train_smot.shape, y_train_smot.shape)\r\n",
        "  return X_train_smot, y_train_smot"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcdoA_OrqRO9"
      },
      "source": [
        "# Feature Extraction\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic_ZeMddBFaf"
      },
      "source": [
        "def print_embedding_shape(X_train, y_train, X_val, y_val, X_test, y_test):\r\n",
        "  print('Shape of Training and Labelled data after random oversampling:', X_train.shape, y_train.shape)\r\n",
        "  # print('Shape of Validation and Labelled data after random oversampling:', X_val.shape, y_val.shape)\r\n",
        "  # print('Shape of Test and Labelled data after random oversampling:', X_test.shape, y_test.shape)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F630YlTXPF7H"
      },
      "source": [
        "##Word Embedding\r\n",
        "---Tranforming Text to Vectors\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9Y3PWLf_Nea"
      },
      "source": [
        "###Count Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbn9VMjCEilt"
      },
      "source": [
        "#### Total features\r\n",
        "\r\n",
        "*   n-gram - 37221\r\n",
        "*   bi-grams - 328966\r\n",
        "*   tri-grams - 682076\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twuq7y9SqRO9"
      },
      "source": [
        "# Transformin Dataset into a vector on the basis of the frequency (count) of each word that occurs in the entire text.\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "def embedding_count_vectorize(X_train, y_train, X_val, y_val, X_test, y_test, ngram=1):\r\n",
        "  # Create Vector object\r\n",
        "  countvector=CountVectorizer(ngram_range=(1,ngram)) # max_features = default, ngrams_range=(1,1)\r\n",
        "\r\n",
        "  # Fit and Transform Train data\r\n",
        "  X_train=countvector.fit_transform(X_train)\r\n",
        "\r\n",
        "  # Printing the identified Unique words along with trigrams \r\n",
        "  vocab_dict = countvector.vocabulary_\r\n",
        "  # print('Sample Vocabulary',{k: vocab_dict[k] for k in list(vocab_dict)[:20]})\r\n",
        "\r\n",
        "  # Summarizing the Encoded Texts \r\n",
        "  # print(\"Encoded Document is:\") \r\n",
        "  # print(countvector.toarray())\r\n",
        "\r\n",
        "  # Transform Validation dataset\r\n",
        "  X_val = countvector.transform(val_df['processed_tweets'])\r\n",
        "  y_val = val_df.label\r\n",
        "\r\n",
        "  # Transform Test dataset\r\n",
        "  X_test = countvector.transform(test_df['processed_tweets'])\r\n",
        "  y_test = test_df.label\r\n",
        "\r\n",
        "  print_embedding_shape(X_train, y_train, X_val, y_val, X_test, y_test)\r\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23Jc40uoPWw-"
      },
      "source": [
        "###TF-IDF\r\n",
        "\r\n",
        "#### Total features\r\n",
        "\r\n",
        "*   n-gram - 37221\r\n",
        "*   bi-grams - 328966\r\n",
        "*   tri-grams - 682076\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRFIWEPg6ZAc"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "def embedding_tfidf(X_train, y_train, X_val, y_val, X_test, y_test, ngram=1):\r\n",
        "\r\n",
        "  tokenization = TfidfVectorizer(ngram_range=(1,ngram)) # max_features=6135\r\n",
        "  X_train = tokenization.fit_transform(X_train)\r\n",
        "  # X_train = X_train.toarray()\r\n",
        "\r\n",
        "  X_val = tokenization.transform(val_df['processed_tweets'])\r\n",
        "  y_val = val_df.label\r\n",
        "\r\n",
        "  # Transform Test dataset\r\n",
        "  X_test = tokenization.transform(test_df['processed_tweets'])\r\n",
        "  y_test = test_df.label\r\n",
        "  print_embedding_shape(X_train, y_train, X_val, y_val, X_test, y_test)\r\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aweC_8ytkHl"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "w2v_model = 0 \r\n",
        "def embedding_Word2Vec(X_train, y_train, X_val, y_val, X_test, y_test):\r\n",
        "\r\n",
        "  X_train = pd.DataFrame(X_train, columns=['tweet'])\r\n",
        "  X_train['tokend'] = X_train['tweet'].apply(word_tokenize)\r\n",
        "\r\n",
        "  X_val = pd.DataFrame(X_val, columns=['tweet'])\r\n",
        "  X_val['tokend'] = X_val['tweet'].apply(word_tokenize)\r\n",
        "\r\n",
        "  X_test = pd.DataFrame(X_test, columns=['tweet'])\r\n",
        "  X_test['tokend'] = X_test['tweet'].apply(word_tokenize)\r\n",
        "\r\n",
        "\r\n",
        "  # w2v = Word2Vec(X_train.tokend, size=350, window=10, min_count=1, iter=20)\r\n",
        "\r\n",
        "  def text_to_vector(text):\r\n",
        "      \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\r\n",
        "      text = [word for word in text if word in w2v_model.wv.vocab]\r\n",
        "      if not len(text):\r\n",
        "        text = ['tweet']\r\n",
        "      return np.mean(w2v_model[text], axis=0)\r\n",
        "\r\n",
        "\r\n",
        "  w2v_model = Word2Vec(X_train.tokend, min_count=20,\r\n",
        "                      window=7,\r\n",
        "                      size=300,\r\n",
        "                      sample=6e-5, \r\n",
        "                      alpha=0.03, \r\n",
        "                      min_alpha=0.0007, \r\n",
        "                      negative=10)\r\n",
        "\r\n",
        "  X_train['doc_vector'] = X_train['tokend'].apply(text_to_vector)\r\n",
        "  X_val['doc_vector'] = X_val['tokend'].apply(text_to_vector)\r\n",
        "  X_test['doc_vector'] = X_test['tokend'].apply(text_to_vector)\r\n",
        "\r\n",
        "  x_train = list(X_train['doc_vector'])\r\n",
        "  x_val = list(X_val['doc_vector'])\r\n",
        "  x_test = list(X_test['doc_vector'])\r\n",
        "  train_model(x_train, y_train, x_val, y_val, x_test, y_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4BzAike4mLr"
      },
      "source": [
        "# pip install zeugma"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuInbXvB3Jsb"
      },
      "source": [
        "from zeugma.embeddings import EmbeddingTransformer\r\n",
        "\r\n",
        "def embedding_Glove(X_train, y_train, X_val, y_val, X_test, y_test):\r\n",
        "    glove = EmbeddingTransformer('glove')\r\n",
        "    x_train = glove.transform(X_train)\r\n",
        "    x_val = glove.transform(X_val)\r\n",
        "    x_test = glove.transform(X_test)\r\n",
        "    \r\n",
        "    train_model(x_train, y_train, x_val, y_val, x_test, y_test)\r\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eP0Ro5AKBpU"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTRIb9ZvkY2F"
      },
      "source": [
        "#Get Prepared Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQAH8J7Di2Qp"
      },
      "source": [
        "def get_dataset():\r\n",
        "  X = train_df.processed_tweets.values\r\n",
        "  y = train_df.label.values\r\n",
        "\r\n",
        "  X_train, y_train = smot_oversmapling(X, y)\r\n",
        "\r\n",
        "  X_val = val_df.processed_tweets.values\r\n",
        "  y_val = val_df.label.values\r\n",
        "\r\n",
        "  X_test = test_df.processed_tweets.values\r\n",
        "  y_test = test_df.label.values\r\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rc1Q8nTqRO-"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wHMIn8PlnWU"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "def logistic_classifier(X_train, y_train):\r\n",
        "  \"\"\" \r\n",
        "    Initialize LogisticRegression Object. \r\n",
        "\r\n",
        "    This function initializes the parameters on which Grid Search has to be \r\n",
        "    performed and calls the api to Grid Search, which returns the best\r\n",
        "    estimator for LogisticRegression. \r\n",
        "\r\n",
        "    Parameters: \r\n",
        "    X_train (DataFrame): Input Training Data set  \r\n",
        "    y_train (DataFrame): Labelled Training Data \r\n",
        "    X_test (DataFrame): Input Validation Data set\r\n",
        "    y_test (DataFrame): Labelled Validation Data\r\n",
        "\r\n",
        "\r\n",
        "    Returns: \r\n",
        "    classifier_lr: Best esitmator for LogisticRegression \r\n",
        "\r\n",
        "  \"\"\"    \r\n",
        "  model = LogisticRegression(max_iter=100)\r\n",
        "  # model = LinearSVC(C=0.5, random_state=70, max_iter=2000)\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIpUyBjmmcXH"
      },
      "source": [
        "from sklearn.metrics import recall_score, f1_score\r\n",
        "\r\n",
        "def fit_models(classifier, X_train, y_train, X_val, y_val, X_test, y_test):\r\n",
        "    \"\"\" \r\n",
        "      Wrapper to train and Validate Classifiers. \r\n",
        "\r\n",
        "      This is a wrapper function which trains the classifer, gets the test \r\n",
        "      score, performs k-fold cross validation with 10 splits.\r\n",
        "\r\n",
        "      Parameters: \r\n",
        "      X_train (DataFrame): Input Training Data set  \r\n",
        "      y_train (DataFrame): Labelled Training Data \r\n",
        "      X_test (DataFrame): Input Validation Data set\r\n",
        "      y_test (DataFrame): Labelled Validation Data\r\n",
        "\r\n",
        "      Returns: \r\n",
        "      test_score: Score of prediction done on test data\r\n",
        "      accuracy_mean: Average Cross Validation Score\r\n",
        "      cross_valScore: List of all the Cross Validation Score\r\n",
        "\r\n",
        "    \"\"\"     \r\n",
        "    classifier.fit(X_train, y_train)\r\n",
        "\r\n",
        "    validation_score = classifier.score(X_val,y_val)\r\n",
        "    test_score = classifier.score(X_test,y_test)\r\n",
        "\r\n",
        "    y_val_pred = classifier.predict(X_val)\r\n",
        "    recallScore_val = recall_score(y_val, y_val_pred, average='macro')\r\n",
        "    f1Score_val = f1_score(y_val, y_val_pred, average='macro')\r\n",
        "\r\n",
        "    y_test_pred = classifier.predict(X_test)\r\n",
        "    recallScore_test = recall_score(y_test, y_test_pred, average='macro')\r\n",
        "    f1Score_test = f1_score(y_test, y_test_pred, average='macro')\r\n",
        "\r\n",
        "\r\n",
        "    return validation_score, recallScore_val, test_score, recallScore_test, f1Score_val, f1Score_test"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQyhYqNpeCsi"
      },
      "source": [
        "def train_model(X_train, y_train, X_val, y_val, X_test, y_test):\r\n",
        "  classifier_lr = logistic_classifier(X_train, y_train)\r\n",
        "  validation_score, recallScore_val, test_score, recallScore_test, f1Score_val, f1Score_test = fit_models(classifier_lr, X_train, y_train, X_val, y_val, X_test, y_test)\r\n",
        "  print('Validation Score: {}\\nTest Score: {}'\r\n",
        "    .format(validation_score,test_score)) \r\n",
        "  print('Validation Recall Score: {}\\nTest Recall Score: {}'\r\n",
        "    .format(recallScore_val, recallScore_test)) \r\n",
        "  print('Validation F1 Score: {}\\nTest F1 Score: {}'\r\n",
        "    .format(f1Score_val, f1Score_test)) "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJb-QVMmnD3N"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVn0BS3tpSuY"
      },
      "source": [
        "def embedding_count_vectorize_ngram(X_train, y_train, X_val, y_val, X_test, y_test):\r\n",
        "  for ngram in range(1,4):\r\n",
        "    print('N-GRAM: {}'.format(ngram))\r\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = get_dataset()\r\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = embedding_count_vectorize(X_train, y_train, X_val, y_val, X_test, y_test, ngram)\r\n",
        "    train_model(X_train, y_train, X_val, y_val, X_test, y_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8x-fOzbqq8H"
      },
      "source": [
        "def embedding_tfid_ngram(X_train, y_train, X_val, y_val, X_test, y_test):\r\n",
        "  for ngram in range(1,4):\r\n",
        "    print('N-GRAM: {}'.format(ngram))\r\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = get_dataset()\r\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = embedding_tfidf(X_train, y_train, X_val, y_val, X_test, y_test, ngram)\r\n",
        "    train_model(X_train, y_train, X_val, y_val, X_test, y_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-M4u1vrnCNn"
      },
      "source": [
        "def init_training(features_extraction):\r\n",
        "  X_train, y_train, X_val, y_val, X_test, y_test = get_dataset()\r\n",
        "  feature_extraction(X_train, y_train, X_val, y_val, X_test, y_test)\r\n",
        "  # train_model(X_train, y_train, X_val, y_val, X_test, y_test)  "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "653owJmLuxGn",
        "outputId": "ab10cbfd-2934-40dd-9143-c376c535747f"
      },
      "source": [
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "# feature_extraction_strategy = [embedding_tfid_ngram, embedding_count_vectorize_ngram, embedding_Glove, embedding_Word2Vec]\r\n",
        "feature_extraction_strategy = [embedding_Glove]\r\n",
        "classification_task = {'SENTIMENT_ANALYSIS' : 'Sentiment_analysis',\r\n",
        "                       'HATE_ANALYSIS' : 'hate_analysis',\r\n",
        "                       'OFFENSIVE_LANGUAGE' : 'offensive_analysis',\r\n",
        "                       'IRONY_ANALYSIS' : 'irony_analysis'\r\n",
        "                       }\r\n",
        "\r\n",
        "for key, task in classification_task.items():\r\n",
        "  print('=========================================')\r\n",
        "  print('CLASSIFICATION TASK: {}'.format(key))\r\n",
        "  print('=========================================')\r\n",
        "  if key == 'SENTIMENT_ANALYSIS':\r\n",
        "    continue\r\n",
        "    train_df, val_df, test_df = prepare_dataset(SENTIMENT_TRAIN_TEXT, SENTIMENT_TRAIN_LABEL,\r\n",
        "                        SENTIMENT_VALIDATION_TEXT, SENTIMENT_VALIDATION_LABEL,\r\n",
        "                        SENTIMENT_TEST_TEXT, SENTIMENT_TEST_LABEL, classification_task['SENTIMENT_ANALYSIS']\r\n",
        "                        )\r\n",
        "  if key == 'HATE_ANALYSIS':\r\n",
        "    # continue\r\n",
        "    train_df, val_df, test_df = prepare_dataset(HATE_TRAIN_TEXT, HATE_TRAIN_LABEL,\r\n",
        "                        HATE_VALIDATION_TEXT, HATE_VALIDATION_LABEL,\r\n",
        "                        HATE_TEST_TEXT, HATE_TEST_LABEL, classification_task['HATE_ANALYSIS']\r\n",
        "                        )\r\n",
        "  if key == 'OFFENSIVE_LANGUAGE':\r\n",
        "    continue\r\n",
        "    train_df, val_df, test_df = prepare_dataset(OFFENSE_TRAIN_TEXT, OFFENSE_TRAIN_LABEL,\r\n",
        "                        OFFENSE_VALIDATION_TEXT, OFFENSE_VALIDATION_LABEL,\r\n",
        "                        OFFENSE_TEST_TEXT, OFFENSE_TEST_LABEL, classification_task['OFFENSIVE_LANGUAGE']\r\n",
        "                        )\r\n",
        "  if key == 'IRONY_ANALYSIS':\r\n",
        "    continue\r\n",
        "    train_df, val_df, test_df = prepare_dataset(OFFENSE_TRAIN_TEXT, OFFENSE_TRAIN_LABEL,\r\n",
        "                        OFFENSE_VALIDATION_TEXT, OFFENSE_VALIDATION_LABEL,\r\n",
        "                        OFFENSE_TEST_TEXT, OFFENSE_TEST_LABEL, classification_task['IRONY_ANALYSIS']\r\n",
        "                        )    \r\n",
        "  for feature_extraction in feature_extraction_strategy:\r\n",
        "    print('FEATURE EXTRACTION STRATEGY TASK: {}'.format(feature_extraction))\r\n",
        "    init_training(feature_extraction)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================\n",
            "CLASSIFICATION TASK: SENTIMENT_ANALYSIS\n",
            "=========================================\n",
            "=========================================\n",
            "CLASSIFICATION TASK: HATE_ANALYSIS\n",
            "=========================================\n",
            "FEATURE EXTRACTION STRATEGY TASK: <function embedding_Glove at 0x7f8e97f42c20>\n",
            "Validation Score: 0.651\n",
            "Test Score: 0.5595959595959596\n",
            "Validation Recall Score: 0.6396691884203686\n",
            "Test Recall Score: 0.5865023413062964\n",
            "Validation F1 Score: 0.6404874948365145\n",
            "Test F1 Score: 0.5567975371708405\n",
            "=========================================\n",
            "CLASSIFICATION TASK: OFFENSIVE_LANGUAGE\n",
            "=========================================\n",
            "=========================================\n",
            "CLASSIFICATION TASK: IRONY_ANALYSIS\n",
            "=========================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3xGQfAGPTnf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}