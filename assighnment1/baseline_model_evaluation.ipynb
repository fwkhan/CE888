{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_model_evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwkhan/CE888/blob/main/assighnment1/baseline_model_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2snf_JzoqiLq"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk4JkCKPoEwY"
      },
      "source": [
        "# import torch\r\n",
        "# if torch.cuda.is_available():       \r\n",
        "#     device = torch.device(\"cuda\")\r\n",
        "#     print(f'There are {torch.cuda.device_count()} GPU(s) available.')\r\n",
        "#     print('Device name:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "# else:\r\n",
        "#     print('No GPU available, using the CPU instead.')\r\n",
        "#     device = torch.device(\"cpu\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPFoHm-Td2zu"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "# Marry nltk and ScikitLearn\r\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\r\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\r\n",
        "import pickle\r\n",
        "from nltk import word_tokenize\r\n",
        "import nltk\r\n",
        "import random\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.classify import ClassifierI\r\n",
        "from statistics import mode\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import seaborn as sns\r\n",
        "import requests\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjorzjloqRO8"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA-D50_uaK7f"
      },
      "source": [
        "############################ SENTIMENT ANALYSIS #################################################\r\n",
        "SENTIMENT_TRAIN_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_text.txt'\r\n",
        "SENTIMENT_VALIDATION_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_text.txt'\r\n",
        "SENTIMENT_TEST_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/test_text.txt'\r\n",
        "\r\n",
        "SENTIMENT_TRAIN_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_labels.txt'\r\n",
        "SENTIMENT_VALIDATION_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_labels.txt'\r\n",
        "SENTIMENT_TEST_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/test_labels.txt'\r\n",
        "\r\n",
        "############################ HATE #################################################\r\n",
        "HATE_TRAIN_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_text.txt'\r\n",
        "HATE_VALIDATION_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_text.txt'\r\n",
        "HATE_TEST_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_text.txt'\r\n",
        "\r\n",
        "HATE_TRAIN_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_labels.txt'\r\n",
        "HATE_VALIDATION_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_labels.txt'\r\n",
        "HATE_TEST_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_labels.txt'\r\n",
        "\r\n",
        "############################ OFFENSIVE LANGUAGE#################################################\r\n",
        "OFFENSE_TRAIN_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_text.txt'\r\n",
        "OFFENSE_VALIDATION_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_text.txt'\r\n",
        "OFFENSE_TEST_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_text.txt'\r\n",
        "\r\n",
        "OFFENSE_TRAIN_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_labels.txt'\r\n",
        "OFFENSE_VALIDATION_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_labels.txt'\r\n",
        "OFFENSE_TEST_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_labels.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5_F0_kzZz14"
      },
      "source": [
        "def preprocess(df): \r\n",
        "    lemmatizer  = WordNetLemmatizer()\r\n",
        "    ignore_words = ['user', 'st'] \r\n",
        "    df['processed_tweets'] = df['tweet'].replace('[^a-zA-Z]',' ', regex=True,\r\n",
        "                                                  inplace=False)\r\n",
        "    df['processed_tweets'] = df['processed_tweets'].apply(lambda x: [w.lower() for w in x.split()])\r\n",
        "    df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ([word for word in tweet if not word in stopwords.words(\"english\")]))\r\n",
        "    df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ([lemmatizer.lemmatize(word) for word in tweet]))\r\n",
        "\r\n",
        "\r\n",
        "    df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ' '.join([word for word in tweet if len(word)>2]))\r\n",
        "\r\n",
        "    df['processed_tweets'] = df['processed_tweets'].apply(lambda x: ' '.join([word for word in x.split() if not word in ignore_words]))\r\n",
        "    \r\n",
        "    df[\"sentence_length\"] = df.tweet.apply(lambda x: len(str(x).split()))\r\n",
        "    return df\r\n",
        "\r\n",
        "\r\n",
        "# Wrapper to convert text data to pandas Dataframe\r\n",
        "def txt_to_df(data, label, classification_task):\r\n",
        "    tweet = []\r\n",
        "    sentiments = []\r\n",
        "    for sentence in data.split('\\n'):\r\n",
        "        tweet.append(sentence)\r\n",
        "    for sentiment in label.split('\\n'):\r\n",
        "        try:\r\n",
        "            sentiments.append(int(sentiment))\r\n",
        "        except ValueError:\r\n",
        "            pass\r\n",
        "    df= pd.DataFrame(tweet[:-1], columns=['tweet'])\r\n",
        "    df['label'] = sentiments\r\n",
        "    if classification_task == 'Sentiment_analysis':\r\n",
        "      df['sentiment'] = df.label.apply(lambda x: 'Negative'if x==0 else ('Neutral' if x==1 else 'Positive'))\r\n",
        "    if classification_task == 'hate_analysis':\r\n",
        "      df['sentiment'] = df.label.apply(lambda x: 'Not-hate'if x==0 else 'hate')\r\n",
        "    if classification_task == 'offensive_analysis':\r\n",
        "      df['sentiment'] = df.label.apply(lambda x: 'Not-offensive 'if x==0 else 'offensive')\r\n",
        "    return df\r\n",
        "\r\n",
        "\r\n",
        "def prepare_dataset(TRAIN_TEXT, TRAIN_LABEL, VAL_TEXT, VAL_LABEL, TEST_TEXT, TEST_LABEL, classification_task):\r\n",
        "  # Reading Train, Vvalidation & Test data from tweeteval Github Repo.\r\n",
        "  train_tweets_txt = requests.get(TRAIN_TEXT).text\r\n",
        "  train_labels_txt = requests.get(TRAIN_LABEL).text\r\n",
        "\r\n",
        "  val_tweets_txt = requests.get(VAL_TEXT).text\r\n",
        "  val_labels_txt = requests.get(VAL_LABEL).text\r\n",
        "\r\n",
        "  test_tweets_txt = requests.get(TEST_TEXT).text\r\n",
        "  test_labels_txt = requests.get(TEST_LABEL).text\r\n",
        "\r\n",
        "  # Converting text data to pandas Dataframe\r\n",
        "  train_df = txt_to_df(train_tweets_txt, train_labels_txt, classification_task)\r\n",
        "  val_df = txt_to_df(val_tweets_txt, val_labels_txt, classification_task)\r\n",
        "  test_df = txt_to_df(test_tweets_txt, test_labels_txt, classification_task)\r\n",
        "\r\n",
        "  train_df = preprocess(train_df)\r\n",
        "  val_df = preprocess(val_df)\r\n",
        "  test_df = preprocess(test_df)  \r\n",
        "\r\n",
        "  return train_df, val_df, test_df\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TA2OA-k5_ye"
      },
      "source": [
        "#Random OverSampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xzJ-qQ15-Ii"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\r\n",
        "\r\n",
        "def smot_oversmapling(X, y):\r\n",
        "  X = X.reshape(-1,1)\r\n",
        "  oversample = RandomOverSampler()\r\n",
        "\r\n",
        "  X_train_smot, y_train_smot = oversample.fit_resample(X, y)\r\n",
        "  X_train_smot = X_train_smot.reshape(len(X_train_smot),)\r\n",
        "  # print('Shape of Training and Labelled data after random oversampling:', X_train_smot.shape, y_train_smot.shape)\r\n",
        "  return X_train_smot, y_train_smot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcdoA_OrqRO9"
      },
      "source": [
        "# Feature Extraction\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic_ZeMddBFaf"
      },
      "source": [
        "def print_embedding_shape(X_train, y_train, X_val, y_val, X_test, y_test):\r\n",
        "  print('Shape of Training and Labelled data after random oversampling:', X_train.shape, y_train.shape)\r\n",
        "  # print('Shape of Validation and Labelled data after random oversampling:', X_val.shape, y_val.shape)\r\n",
        "  # print('Shape of Test and Labelled data after random oversampling:', X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F630YlTXPF7H"
      },
      "source": [
        "##Word Embedding\r\n",
        "---Tranforming Text to Vectors\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9Y3PWLf_Nea"
      },
      "source": [
        "###Count Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbn9VMjCEilt"
      },
      "source": [
        "#### Total features\r\n",
        "\r\n",
        "*   n-gram - 37221\r\n",
        "*   bi-grams - 328966\r\n",
        "*   tri-grams - 682076\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twuq7y9SqRO9"
      },
      "source": [
        "# Transformin Dataset into a vector on the basis of the frequency (count) of each word that occurs in the entire text.\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "def embedding_count_vectorize(X_train, y_train, X_val, y_val, X_test, y_test, ngram=1):\r\n",
        "  # Create Vector object\r\n",
        "  countvector=CountVectorizer(ngram_range=(1,ngram)) # max_features = default, ngrams_range=(1,1)\r\n",
        "\r\n",
        "  # Fit and Transform Train data\r\n",
        "  X_train=countvector.fit_transform(X_train)\r\n",
        "  y_train = y_train\r\n",
        "\r\n",
        "  # Printing the identified Unique words along with trigrams \r\n",
        "  vocab_dict = countvector.vocabulary_\r\n",
        "  # print('Sample Vocabulary',{k: vocab_dict[k] for k in list(vocab_dict)[:20]})\r\n",
        "\r\n",
        "  # Summarizing the Encoded Texts \r\n",
        "  # print(\"Encoded Document is:\") \r\n",
        "  # print(countvector.toarray())\r\n",
        "\r\n",
        "  # Transform Validation dataset\r\n",
        "  X_val = countvector.transform(val_df['processed_tweets'])\r\n",
        "  y_val = val_df.label\r\n",
        "\r\n",
        "  # Transform Test dataset\r\n",
        "  X_test = countvector.transform(test_df['processed_tweets'])\r\n",
        "  y_test = test_df.label\r\n",
        "\r\n",
        "  print_embedding_shape(X_train, y_train, X_val, y_val, X_test, y_test)\r\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23Jc40uoPWw-"
      },
      "source": [
        "###TF-IDF\r\n",
        "\r\n",
        "#### Total features\r\n",
        "\r\n",
        "*   n-gram - 37221\r\n",
        "*   bi-grams - 328966\r\n",
        "*   tri-grams - 682076\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRFIWEPg6ZAc"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "def embedding_tfidf(X_train, y_train, X_val, y_val, X_test, y_test, ngram=1):\r\n",
        "\r\n",
        "  tokenization = TfidfVectorizer(ngram_range=(1,ngram)) # max_features=6135\r\n",
        "  X_train = tokenization.fit_transform(X_train)\r\n",
        "  y_train = y_train\r\n",
        "  # X_train = X_train.toarray()\r\n",
        "\r\n",
        "  X_val = tokenization.transform(val_df['processed_tweets'])\r\n",
        "  y_val = val_df.label\r\n",
        "\r\n",
        "  # Transform Test dataset\r\n",
        "  X_test = tokenization.transform(test_df['processed_tweets'])\r\n",
        "  y_test = test_df.label\r\n",
        "  print_embedding_shape(X_train, y_train, X_val, y_val, X_test, y_test)\r\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aweC_8ytkHl"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "w2v_model = 0 \r\n",
        "def embedding_Word2Vec(X_train, y_train, X_val, y_val, X_test, y_test):\r\n",
        "\r\n",
        "  X_train = pd.DataFrame(X_train, columns=['tweet'])\r\n",
        "  X_train['tokend'] = X_train['tweet'].apply(word_tokenize)\r\n",
        "\r\n",
        "  X_val = pd.DataFrame(X_val, columns=['tweet'])\r\n",
        "  X_val['tokend'] = X_val['tweet'].apply(word_tokenize)\r\n",
        "\r\n",
        "  X_test = pd.DataFrame(X_test, columns=['tweet'])\r\n",
        "  X_test['tokend'] = X_test['tweet'].apply(word_tokenize)\r\n",
        "\r\n",
        "\r\n",
        "  # w2v = Word2Vec(X_train.tokend, size=350, window=10, min_count=1, iter=20)\r\n",
        "\r\n",
        "  def text_to_vector(text):\r\n",
        "      \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\r\n",
        "      text = [word for word in text if word in w2v_model.wv.vocab]\r\n",
        "      if not len(text):\r\n",
        "        text = ['tweet']\r\n",
        "      return np.mean(w2v_model[text], axis=0)\r\n",
        "\r\n",
        "\r\n",
        "  w2v_model = Word2Vec(X_train.tokend, min_count=20,\r\n",
        "                      window=7,\r\n",
        "                      size=300,\r\n",
        "                      sample=6e-5, \r\n",
        "                      alpha=0.03, \r\n",
        "                      min_alpha=0.0007, \r\n",
        "                      negative=10)\r\n",
        "\r\n",
        "  X_train['doc_vector'] = X_train['tokend'].apply(text_to_vector)\r\n",
        "  X_val['doc_vector'] = X_val['tokend'].apply(text_to_vector)\r\n",
        "  X_test['doc_vector'] = X_test['tokend'].apply(text_to_vector)\r\n",
        "\r\n",
        "  y_train = y_train\r\n",
        "  y_test = y_test\r\n",
        "  y_val = y_val\r\n",
        "\r\n",
        "  x_train = list(X_train['doc_vector'])\r\n",
        "  x_val = list(X_val['doc_vector'])\r\n",
        "  x_test = list(X_test['doc_vector'])\r\n",
        "  train_model(x_train, y_train, x_val, y_val, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTRIb9ZvkY2F"
      },
      "source": [
        "#Get Prepared Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQAH8J7Di2Qp"
      },
      "source": [
        "def get_dataset():\r\n",
        "  X = train_df.processed_tweets.values\r\n",
        "  y = train_df.label.values\r\n",
        "\r\n",
        "  X_train, y_train = smot_oversmapling(X, y)\r\n",
        "\r\n",
        "  X_val = val_df.processed_tweets.values\r\n",
        "  y_val = val_df.label.values\r\n",
        "\r\n",
        "  X_test = test_df.processed_tweets.values\r\n",
        "  y_test = test_df.label.values\r\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rc1Q8nTqRO-"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wHMIn8PlnWU"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "def logistic_classifier(X_train, y_train):\r\n",
        "  \"\"\" \r\n",
        "    Initialize LogisticRegression Object. \r\n",
        "\r\n",
        "    This function initializes the parameters on which Grid Search has to be \r\n",
        "    performed and calls the api to Grid Search, which returns the best\r\n",
        "    estimator for LogisticRegression. \r\n",
        "\r\n",
        "    Parameters: \r\n",
        "    X_train (DataFrame): Input Training Data set  \r\n",
        "    y_train (DataFrame): Labelled Training Data \r\n",
        "    X_test (DataFrame): Input Validation Data set\r\n",
        "    y_test (DataFrame): Labelled Validation Data\r\n",
        "\r\n",
        "\r\n",
        "    Returns: \r\n",
        "    classifier_lr: Best esitmator for LogisticRegression \r\n",
        "\r\n",
        "  \"\"\"    \r\n",
        "  model = LogisticRegression(max_iter=100)\r\n",
        "  # model = LinearSVC(C=0.5, random_state=70, max_iter=2000)\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIpUyBjmmcXH"
      },
      "source": [
        "from sklearn.metrics import recall_score, f1_score\r\n",
        "\r\n",
        "def fit_models(classifier, X_train, y_train, X_val, y_val, X_test, y_test):\r\n",
        "    \"\"\" \r\n",
        "      Wrapper to train and Validate Classifiers. \r\n",
        "\r\n",
        "      This is a wrapper function which trains the classifer, gets the test \r\n",
        "      score, performs k-fold cross validation with 10 splits.\r\n",
        "\r\n",
        "      Parameters: \r\n",
        "      X_train (DataFrame): Input Training Data set  \r\n",
        "      y_train (DataFrame): Labelled Training Data \r\n",
        "      X_test (DataFrame): Input Validation Data set\r\n",
        "      y_test (DataFrame): Labelled Validation Data\r\n",
        "\r\n",
        "      Returns: \r\n",
        "      test_score: Score of prediction done on test data\r\n",
        "      accuracy_mean: Average Cross Validation Score\r\n",
        "      cross_valScore: List of all the Cross Validation Score\r\n",
        "\r\n",
        "    \"\"\"     \r\n",
        "    classifier.fit(X_train, y_train)\r\n",
        "\r\n",
        "    validation_score = classifier.score(X_val,y_val)\r\n",
        "    test_score = classifier.score(X_test,y_test)\r\n",
        "\r\n",
        "    y_val_pred = classifier.predict(X_val)\r\n",
        "    recallScore_val = recall_score(y_val, y_val_pred, average='macro')\r\n",
        "    f1Score_val = f1_score(y_val, y_val_pred, average='macro')\r\n",
        "\r\n",
        "    y_test_pred = classifier.predict(X_test)\r\n",
        "    recallScore_test = recall_score(y_test, y_test_pred, average='macro')\r\n",
        "    f1Score_test = f1_score(y_test, y_test_pred, average='macro')\r\n",
        "\r\n",
        "\r\n",
        "    return validation_score, recallScore_val, test_score, recallScore_test, f1Score_val, f1Score_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQyhYqNpeCsi"
      },
      "source": [
        "def train_model(X_train, y_train, X_val, y_val, X_test, y_test):\r\n",
        "  classifier_lr = logistic_classifier(X_train, y_train)\r\n",
        "  validation_score, recallScore_val, test_score, recallScore_test, f1Score_val, f1Score_test = fit_models(classifier_lr, X_train, y_train, X_val, y_val, X_test, y_test)\r\n",
        "  print('Validation Score: {}\\nTest Score: {}'\r\n",
        "    .format(validation_score,test_score)) \r\n",
        "  print('Validation Recall Score: {}\\nTest Recall Score: {}'\r\n",
        "    .format(recallScore_val, recallScore_test)) \r\n",
        "  print('Validation F1 Score: {}\\nTest F1 Score: {}'\r\n",
        "    .format(f1Score_val, f1Score_test)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJb-QVMmnD3N"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVn0BS3tpSuY"
      },
      "source": [
        "def embedding_count_vectorize_ngram(X_train, y_train, X_val, y_val, X_test, y_test):\r\n",
        "  for ngram in range(1,4):\r\n",
        "    print('N-GRAM: {}'.format(ngram))\r\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = get_dataset()\r\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = embedding_count_vectorize(X_train, y_train, X_val, y_val, X_test, y_test, ngram)\r\n",
        "    train_model(X_train, y_train, X_val, y_val, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8x-fOzbqq8H"
      },
      "source": [
        "def embedding_tfid_ngram(X_train, y_train, X_val, y_val, X_test, y_test):\r\n",
        "  for ngram in range(1,4):\r\n",
        "    print('N-GRAM: {}'.format(ngram))\r\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = get_dataset()\r\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = embedding_tfidf(X_train, y_train, X_val, y_val, X_test, y_test, ngram)\r\n",
        "    train_model(X_train, y_train, X_val, y_val, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-M4u1vrnCNn"
      },
      "source": [
        "def init_training(features_extraction):\r\n",
        "  X_train, y_train, X_val, y_val, X_test, y_test = get_dataset()\r\n",
        "  feature_extraction(X_train, y_train, X_val, y_val, X_test, y_test)\r\n",
        "  # train_model(X_train, y_train, X_val, y_val, X_test, y_test)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "653owJmLuxGn"
      },
      "source": [
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "feature_extraction_strategy = [embedding_tfid_ngram, embedding_count_vectorize_ngram, embedding_Word2Vec]\r\n",
        "# feature_extraction_strategy = [embedding_Word2Vec]\r\n",
        "classification_task = {'SENTIMENT_ANALYSIS' : 'Sentiment_analysis',\r\n",
        "                       'HATE_ANALYSIS' : 'hate_analysis',\r\n",
        "                       'OFFENSIVE_LANGUAGE' : 'offensive_analysis'\r\n",
        "                       }\r\n",
        "\r\n",
        "for key, task in classification_task.items():\r\n",
        "  print('=========================================')\r\n",
        "  print('CLASSIFICATION TASK: {}'.format(key))\r\n",
        "  print('=========================================')\r\n",
        "  if key == 'SENTIMENT_ANALYSIS':\r\n",
        "    train_df, val_df, test_df = prepare_dataset(SENTIMENT_TRAIN_TEXT, SENTIMENT_TRAIN_LABEL,\r\n",
        "                        SENTIMENT_VALIDATION_TEXT, SENTIMENT_VALIDATION_LABEL,\r\n",
        "                        SENTIMENT_TEST_TEXT, SENTIMENT_TEST_LABEL, classification_task['SENTIMENT_ANALYSIS']\r\n",
        "                        )\r\n",
        "  if key == 'HATE_ANALYSIS':\r\n",
        "    train_df, val_df, test_df = prepare_dataset(HATE_TRAIN_TEXT, HATE_TRAIN_LABEL,\r\n",
        "                        HATE_VALIDATION_TEXT, HATE_VALIDATION_LABEL,\r\n",
        "                        HATE_TEST_TEXT, HATE_TEST_LABEL, classification_task['HATE_ANALYSIS']\r\n",
        "                        )\r\n",
        "  if key == 'OFFENSIVE_LANGUAGE':\r\n",
        "    train_df, val_df, test_df = prepare_dataset(OFFENSE_TRAIN_TEXT, OFFENSE_TRAIN_LABEL,\r\n",
        "                        OFFENSE_VALIDATION_TEXT, OFFENSE_VALIDATION_LABEL,\r\n",
        "                        OFFENSE_TEST_TEXT, OFFENSE_TEST_LABEL, classification_task['OFFENSIVE_LANGUAGE']\r\n",
        "                        )\r\n",
        "  for feature_extraction in feature_extraction_strategy:\r\n",
        "    print('FEATURE EXTRACTION STRATEGY TASK: {}'.format(feature_extraction))\r\n",
        "    init_training(feature_extraction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9HNOD6Z57kn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}