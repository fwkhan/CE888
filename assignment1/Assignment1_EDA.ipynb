{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1-EDA",
      "provenance": [],
      "collapsed_sections": [
        "VXEu82YYZjpR"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwkhan/CE888/blob/main/assignment1/Assignment1_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qyo2X_VgOLI"
      },
      "source": [
        "# This Scrip contains all the wrapper functions for performing EDA. \r\n",
        "#For all the three tasks EDA is done through this Script. I have created wrapper functions for that, so all the plots and figures are displayed at the end of the script when the perform_eda() api is invoked. I have provided option to perform all the EDA in for loop for all the three dataset; however, that makes visualization difficult. Therefore, I have also provided option to perform EDA for all the three data sets separately for better visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP86rh1kb7T7"
      },
      "source": [
        "!pip install --upgrade plotly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2XZcgzRastY"
      },
      "source": [
        "# Downloading necessities for nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8oc23Nm0cL-"
      },
      "source": [
        "  import nltk\r\n",
        "  nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXEu82YYZjpR"
      },
      "source": [
        "# Importing Necesseties "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agFFVKotZjpS"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import requests\n",
        "import numpy as np \n",
        "import random\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from plotly import graph_objs as go\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "from collections import Counter\n",
        "\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import spacy\n",
        "from spacy.util import compounding\n",
        "from spacy.util import minibatch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Hardcoding commands for print beautification\n",
        "print_format_tab_start = '\\t \\x1b[1;31m'\n",
        "print_format_start = '\\x1b[1;31m'\n",
        "print_format_end = '\\x1b[0m'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuTogDJjQrH0"
      },
      "source": [
        "# Declaring URL link related to all the three datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4o1T71yo3cd"
      },
      "source": [
        "# **Sentiment Analysi, Hate, Offensive and Irony dataset fron TweetEval**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyRv_nGhQ1lh"
      },
      "source": [
        "############################ SENTIMENT ANALYSIS #################################################\r\n",
        "SENTIMENT_TRAIN_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_text.txt'\r\n",
        "SENTIMENT_VALIDATION_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_text.txt'\r\n",
        "SENTIMENT_TEST_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/test_text.txt'\r\n",
        "\r\n",
        "SENTIMENT_TRAIN_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_labels.txt'\r\n",
        "SENTIMENT_VALIDATION_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_labels.txt'\r\n",
        "SENTIMENT_TEST_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/test_labels.txt'\r\n",
        "\r\n",
        "############################ HATE #################################################\r\n",
        "HATE_TRAIN_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_text.txt'\r\n",
        "HATE_VALIDATION_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_text.txt'\r\n",
        "HATE_TEST_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_text.txt'\r\n",
        "\r\n",
        "HATE_TRAIN_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_labels.txt'\r\n",
        "HATE_VALIDATION_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_labels.txt'\r\n",
        "HATE_TEST_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_labels.txt'\r\n",
        "\r\n",
        "############################ OFFENSIVE LANGUAGE#################################################\r\n",
        "OFFENSE_TRAIN_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_text.txt'\r\n",
        "OFFENSE_VALIDATION_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_text.txt'\r\n",
        "OFFENSE_TEST_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_text.txt'\r\n",
        "\r\n",
        "OFFENSE_TRAIN_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_labels.txt'\r\n",
        "OFFENSE_VALIDATION_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_labels.txt'\r\n",
        "OFFENSE_TEST_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_labels.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EevDzBIVQyvF"
      },
      "source": [
        "#In this part, text files are read from github, converted to pandsas dataframe and then processing is done to get rid of noise in the data. All the special characters are removed, words are lower-cased, lemmatization is done instead of stemming, all the words whose length is less than 2 are filtered, getting rid of 'user' from texts and calcualting the length of each tweet and storing it in dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSBd5mXFQ2n8"
      },
      "source": [
        "def preprocess(df, EDA=False): \r\n",
        "    lemmatizer  = WordNetLemmatizer()\r\n",
        "    ignore_words = ['user', 'st'] \r\n",
        "    # Removing all the words not starting from alphabets A to Z(case insensitive)\r\n",
        "    df['processed_tweets'] = df['tweet'].replace('[^a-zA-Z]',' ', regex=True,\r\n",
        "                                                  inplace=False)\r\n",
        "    # Tokenizing and Converting to lower case.\r\n",
        "    df['processed_tweets'] = df['processed_tweets'].apply(lambda x: [w.lower() for w in x.split()])\r\n",
        "\r\n",
        "    if not EDA: # For EDA, not removing any word from the tweet\r\n",
        "      df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ([word for word in tweet if not word in stopwords.words(\"english\")]))\r\n",
        "      df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ([lemmatizer.lemmatize(word) for word in tweet]))\r\n",
        "    else:\r\n",
        "      df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ' '.join([word for word in tweet if len(word)>2]))\r\n",
        "      df['processed_tweets'] = df['processed_tweets'].apply(lambda x: ' '.join([word for word in x.split() if not word in ignore_words]))\r\n",
        "    # Calculate sentence length for each tweet and store it in data frame\r\n",
        "    df[\"sentence_length\"] = df.tweet.apply(lambda x: len(str(x).split()))\r\n",
        "    return df\r\n",
        "\r\n",
        "\r\n",
        "# Wrapper to convert text data to pandas Dataframe\r\n",
        "def txt_to_df(data, label, classification_task):\r\n",
        "    tweet = []\r\n",
        "    sentiments = []\r\n",
        "    # Split the input string by new line and store in tweet list\r\n",
        "    for sentence in data.split('\\n'):\r\n",
        "        tweet.append(sentence)\r\n",
        "    # Split labels by new line and store it in sentiments list\r\n",
        "    for sentiment in label.split('\\n'):\r\n",
        "        # Exception handling is done since the last line is is empty in th\r\n",
        "        # original text file and we get system error as we try to cast to int.\r\n",
        "        try:\r\n",
        "            sentiments.append(int(sentiment))\r\n",
        "        except ValueError:\r\n",
        "            pass\r\n",
        "    # converting list to dataframe and removing the last blank line\r\n",
        "    df= pd.DataFrame(tweet[:-1], columns=['tweet'])\r\n",
        "    df['label'] = sentiments # this colums contains labels in int format\r\n",
        "    # Converting labels to corresponding string based on the task type.\r\n",
        "    if classification_task == 'Sentiment_analysis':\r\n",
        "      df['sentiment'] = df.label.apply(lambda x: 'Negative'if x==0 else ('Neutral' if x==1 else 'Positive'))\r\n",
        "    if classification_task == 'hate_analysis':\r\n",
        "      df['sentiment'] = df.label.apply(lambda x: 'Not-hate'if x==0 else 'hate')\r\n",
        "    if classification_task == 'offensive_analysis':\r\n",
        "      df['sentiment'] = df.label.apply(lambda x: 'Not-offensive'if x==0 else 'offensive')\r\n",
        "    return df\r\n",
        "\r\n",
        "\r\n",
        "def prepare_dataset(TRAIN_TEXT, TRAIN_LABEL, VAL_TEXT, VAL_LABEL, TEST_TEXT, TEST_LABEL, classification_task, EDA=False):\r\n",
        "  # Reading Train, Vvalidation & Test data from tweeteval Github Repo.\r\n",
        "  train_tweets_txt = requests.get(TRAIN_TEXT).text\r\n",
        "  train_labels_txt = requests.get(TRAIN_LABEL).text\r\n",
        "\r\n",
        "  val_tweets_txt = requests.get(VAL_TEXT).text\r\n",
        "  val_labels_txt = requests.get(VAL_LABEL).text\r\n",
        "\r\n",
        "  test_tweets_txt = requests.get(TEST_TEXT).text\r\n",
        "  test_labels_txt = requests.get(TEST_LABEL).text\r\n",
        "\r\n",
        "  # Converting text data to pandas Dataframe\r\n",
        "  train_df = txt_to_df(train_tweets_txt, train_labels_txt, classification_task)\r\n",
        "  val_df = txt_to_df(val_tweets_txt, val_labels_txt, classification_task)\r\n",
        "  test_df = txt_to_df(test_tweets_txt, test_labels_txt, classification_task)\r\n",
        "\r\n",
        "  # Preporcessing the tweets before performing any further operations,\r\n",
        "  # New column in the data frame is created of processed tweets.\r\n",
        "  train_df = preprocess(train_df, EDA)\r\n",
        "  val_df = preprocess(val_df, EDA)\r\n",
        "  test_df = preprocess(test_df, EDA)  \r\n",
        "\r\n",
        "  return train_df, val_df, test_df\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnGgn6bxZjpg"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5_M6wCqfNIZ"
      },
      "source": [
        "## Wrapper API to plot percentage of each class in the training dataset.\r\n",
        "We do see imbalance the training data set, for which we perform RandomOversampling of the minority class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaueIE7iZjpn"
      },
      "source": [
        "# Plotting frequency of each class in the training data.\n",
        "\n",
        "def plot_class_distribution(data):\n",
        "  print(print_format_tab_start+'Pie Chart of Class distribution in the training dataset'+print_format_end)\n",
        "  sns.set_palette('Pastel1_r')\n",
        "  eda_df = data.groupby('sentiment').count()['tweet'].reset_index().sort_values(by='tweet',ascending=False)\n",
        "  labels = list(eda_df.sentiment)\n",
        "  sizes = list(eda_df.tweet)\n",
        "\n",
        "  fig1, ax1 = plt.subplots()\n",
        "  ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnVknKjegBVx"
      },
      "source": [
        "## Wrapper for Removing stopwords, so that the plot doesn't contain meaningless words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgaJqrxvZjp2"
      },
      "source": [
        "def remove_stopword(x):\n",
        "    return [y for y in x if y not in stopwords.words('english')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzpq5ykmhKGa"
      },
      "source": [
        "## Wrapper to get frequency of most frequent 20 words in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qKWuCXrcnaQ"
      },
      "source": [
        "def Get_word_frequency(data):\r\n",
        "  frequent_words = Counter([item for sublist in data['words'] for item in sublist])\r\n",
        "  frequent_20_words = pd.DataFrame(frequent_words.most_common(20))\r\n",
        "  frequent_20_words.columns = ['Common_words','count']\r\n",
        "  return frequent_20_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30EadH9NhUhE"
      },
      "source": [
        "## Wrapper to PLOT frequency of most frequent 20 words in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi3P_tnjZjp1"
      },
      "source": [
        "def plot_word_frequency(frequent_20_words, sentiment=''):\n",
        "  fig = px.bar(frequent_20_words, x=\"count\", y=\"Common_words\", title=f'Most Frequent {sentiment} Words', orientation='h', width=800, height=500, color='Common_words')\n",
        "  fig.show()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbmeGvMahci4"
      },
      "source": [
        "## Wrapper to PLOT TREE MAP of frequency of most frequent 20 words in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI4eBLA7Zjp2"
      },
      "source": [
        "def plot_tree_map(frequent_20_words, sentiment=''):\n",
        "  fig = px.treemap(frequent_20_words, path=['Common_words'], values='count',title=f'Tree of Most Frequent {sentiment} Words', width=1000, height=500, color='Common_words')\n",
        "  fig.show()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0ginDsKh77J"
      },
      "source": [
        "## Wrapper function that invokes all the above defined plotting APIs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLfEyXbrcW54"
      },
      "source": [
        "def train_data_eda(data):\n",
        "  plot_class_distribution(data)\n",
        "  data['words'] = data['processed_tweets'].apply(lambda x:str(x).split())\n",
        "  data['words'] = data['words'].apply(lambda x:remove_stopword(x))\n",
        "  tweets_word_frequency = Get_word_frequency(data)\n",
        "  tweets_word_frequency.style.background_gradient(cmap='cool_r')\n",
        "  \n",
        "  plot_word_frequency(tweets_word_frequency)\n",
        "  plot_tree_map(tweets_word_frequency)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv4gf_S7Zjp5"
      },
      "source": [
        "## Wrapper function to visualize words in each class - their frequency.\n",
        "It also highlights common words across classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV9buiPQh35W"
      },
      "source": [
        "def plot_words_class_basis(data):\r\n",
        "  # Create list of words specific to Each class\r\n",
        "  class_df_list = [] # list of all the classes DataFrame e.g. negative_df, posivtive_df, neutral_df\r\n",
        "  class_specific_word_frequency = []\r\n",
        "\r\n",
        "  # class_dict[classification_task] this command gives the classificatoin task - EX. HATE, OFFENSIVE etc.\r\n",
        "  # class_dict[classification_task][index] - gives the class in that classification task - EX. HATE, NOT-HATE\r\n",
        "\r\n",
        "  # Creating list of dataframe. List will have 2 or 3 elements, depending on the classificaton task.\r\n",
        "  # EX. First element - dataframe containing all the positive words\r\n",
        "  #    Second element - dataframe containing all the negative words\r\n",
        "\r\n",
        "  # len(class_dict[classification_task]) give the number of classes \r\n",
        "  # EX for sentiment analysis - 3 classes\r\n",
        "  # For other two - 2 classes\r\n",
        "  for i in range(len(class_dict[classification_task])):\r\n",
        "    class_df_list.append(data[data['sentiment']==class_dict[classification_task][i]])\r\n",
        "\r\n",
        "  # Get frequency of each word in the dataframe stored in class_df_list\r\n",
        "  # EX. Frequency of positive words, frequency of Negative words and store in\r\n",
        "  # a new list class_specific_word_frequency.\r\n",
        "  for i in range(len(class_dict[classification_task])):\r\n",
        "      class_specific_word_frequency.append(Get_word_frequency(class_df_list[i]))\r\n",
        "      \r\n",
        "  # Plotting word frequency of each class\r\n",
        "  for i in range(len(class_dict[classification_task])):\r\n",
        "    plot_word_frequency(class_specific_word_frequency[i], class_dict[classification_task][i])\r\n",
        "\r\n",
        "  for i in range(len(class_dict[classification_task])):\r\n",
        "    plot_tree_map(class_specific_word_frequency[i], class_dict[classification_task][i])\r\n",
        "  return class_df_list\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpEaDsl8ZjqF"
      },
      "source": [
        "##WordClouds\n",
        "\n",
        "We will be building wordclouds in the following order:\n",
        "\n",
        "* WordCloud of Neutral Tweets\n",
        "* WordCloud of Positive Tweets\n",
        "* WordCloud of Negative Tweets\n",
        "\n",
        "This gives sense of the frequent words in each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUWRs035oxpd"
      },
      "source": [
        "def flatten_list(l):\r\n",
        "    return [x for y in l for x in y]\r\n",
        "\r\n",
        "# color coding our wordclouds \r\n",
        "def red_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\r\n",
        "    return f\"hsl(0, 100%, {random.randint(25, 75)}%)\" \r\n",
        "\r\n",
        "def green_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\r\n",
        "    return f\"hsl({random.randint(90, 150)}, 100%, 30%)\" \r\n",
        "\r\n",
        "def yellow_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\r\n",
        "    return f\"hsl(42, 100%, {random.randint(25, 50)}%)\" \r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "def generate_word_clouds(class_df_list_flatten):\r\n",
        "    # Display the generated image:\r\n",
        "\r\n",
        "    color_funct_list = [red_color_func, green_color_func, yellow_color_func ] \r\n",
        "\r\n",
        "    fig, axes = plt.subplots(1,len(class_df_list_flatten), figsize=(20,10))\r\n",
        "    wordcloud_class_list = []\r\n",
        "    # Creating WordCloud for all the classes in loop\r\n",
        "    print(print_format_tab_start+'WordCloud For all the Classes'+print_format_end)\r\n",
        "    for i in range(len(class_df_list_flatten)):\r\n",
        "\r\n",
        "      wordcloud_class_list.append(WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(class_df_list_flatten[i])))\r\n",
        "      axes[i].imshow(wordcloud_class_list[i].recolor(color_func=color_funct_list[i], random_state=3), interpolation='bilinear')\r\n",
        "      axes[i].set_title(class_dict[classification_task][i])\r\n",
        "      axes[i].axis(\"off\")\r\n",
        "\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZptLmiq0zIM"
      },
      "source": [
        "def visualize_word_clouds(class_df_list):\r\n",
        "  class_df_list_flatten = []\r\n",
        "  for i in range(len(class_dict[classification_task])):\r\n",
        "    class_df_list_flatten.append(flatten_list(class_df_list[i]['words']))\r\n",
        "  generate_word_clouds(class_df_list_flatten)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvIhyRSvziDD"
      },
      "source": [
        "## Scatter plot using Word2Vec and TSNE to visualize the words with similar context on a scatter plot.\r\n",
        "Example: words like days of the weeks or months in year get plotted together, this gives a sense that Word2Vec does a good job in extracting relationships between words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2ph1HEOZjqS"
      },
      "source": [
        "from gensim.models import word2vec\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "\r\n",
        "def tsne_plot(data):\r\n",
        "    \"Creates and TSNE model and plots it\"\r\n",
        "    print(print_format_tab_start+'3D plot w.r.t word context'+print_format_end)\r\n",
        "    labels = []\r\n",
        "    tokens = []\r\n",
        "    model = word2vec.Word2Vec(data.words, size=300, window=20, min_count=50, workers=4)\r\n",
        "    for word in model.wv.vocab:\r\n",
        "        tokens.append(model[word])\r\n",
        "        labels.append(word)\r\n",
        "    \r\n",
        "    tsne_model = TSNE(perplexity=40, n_components=3, init='pca', n_iter=2500, random_state=23)\r\n",
        "    new_values = tsne_model.fit_transform(tokens)\r\n",
        "\r\n",
        "    x = []\r\n",
        "    y = []\r\n",
        "    for value in new_values:\r\n",
        "        x.append(value[0])\r\n",
        "        y.append(value[1])\r\n",
        "        \r\n",
        "    plt.figure(figsize=(20, 16)) \r\n",
        "    for i in range(len(x)):\r\n",
        "        plt.scatter(x[i],y[i])\r\n",
        "        plt.annotate(labels[i],\r\n",
        "                     xy=(x[i], y[i]),\r\n",
        "                     xytext=(5, 2),\r\n",
        "                     textcoords='offset points',\r\n",
        "                     ha='right',\r\n",
        "                     va='bottom')\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jppMjdapvDff"
      },
      "source": [
        "## Visaualtization using 3D scatter plot of training data.\r\n",
        "This gives a sense of the word distribuiton with in each class. From the plots we get an idea that most of the words are sufficiently classified with very few samples crossing the classifcation boundary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwrS6embxaxQ"
      },
      "source": [
        "# to split 30% to visualize later using PCA\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\r\n",
        "import matplotlib\r\n",
        "import matplotlib.patches as mp\r\n",
        "\r\n",
        "def visualize_class_distribution_3d(data):\r\n",
        "  print(print_format_tab_start+'3D scatter plot Class distribution in training data'+print_format_end)\r\n",
        "\r\n",
        "  sns.set()\r\n",
        "  count_vect = CountVectorizer(ngram_range=(1,3), lowercase=False)\r\n",
        "  _, X_, _, y_, = train_test_split(data.processed_tweets.tolist(), data.sentiment.tolist(), test_size = 0.9)\r\n",
        "  X_ = count_vect.fit_transform(X_)\r\n",
        "  y_ = count_vect.transform(y_)\r\n",
        "  unique_label = class_dict[classification_task]\r\n",
        "\r\n",
        "\r\n",
        "  data_visual = TruncatedSVD(n_components = 3).fit_transform(X_)\r\n",
        "  plt.rcParams[\"figure.figsize\"] = [21, 10]\r\n",
        "  ax = plt.subplot(111, projection='3d')\r\n",
        "  colors = ['red', 'blue', 'green']\r\n",
        "  color_patch = []\r\n",
        "  for i in range(len(class_dict[classification_task])):\r\n",
        "    color_patch.append(mp.Patch(color=colors[i],label=class_dict[classification_task][i]))\r\n",
        "  current_palette = sns.color_palette()\r\n",
        "  x = data_visual[:,0]\r\n",
        "  y = data_visual[:, 1]\r\n",
        "  z =  data_visual[:, 2]\r\n",
        "  for no, _ in enumerate(np.unique(y_)):\r\n",
        "      ax.scatter3D(x, y, z, c=x, label = unique_label[no], alpha = 0.5,\r\n",
        "                  cmap=matplotlib.colors.ListedColormap(colors))\r\n",
        "      \r\n",
        "  box = ax.get_position()\r\n",
        "  ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 1])\r\n",
        "  ax.legend(handles=color_patch, loc = 'upper center', fancybox = True, shadow = True, ncol = 3)\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZQ11RtNvfsL"
      },
      "source": [
        "## Wrapper function that invokes all the above defined APIs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8B6j1J5_tx5"
      },
      "source": [
        "def perform_eda(training_data):\r\n",
        "  data = train_data_eda(training_data)\r\n",
        "  class_df_list = plot_words_class_basis(data)\r\n",
        "  visualize_word_clouds(class_df_list)\r\n",
        "  tsne_plot(training_data)\r\n",
        "  visualize_class_distribution_3d(training_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6GRd2xkvlvq"
      },
      "source": [
        "# Starting point of the script:\r\n",
        "We declare classification task dictionary, over which we iterate for all the three tasks.\r\n",
        "\r\n",
        "We, also declare class dictionary, which containes all the classes specific to a given task as values.\r\n",
        "\r\n",
        "TRAIN_ALL: Flag to perform EDA for all the three tasks in loop, by default it is set to 'False' as we do the visualization separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD7Sz4mmzOcu"
      },
      "source": [
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "TRAIN_ALL = False\r\n",
        "#classification task dictionary, over which we iterate for all the three tasks.\r\n",
        "classification_task_dict = {'SENTIMENT_ANALYSIS' : 'Sentiment_analysis',\r\n",
        "                       'HATE_ANALYSIS' : 'hate_analysis',\r\n",
        "                       'OFFENSIVE_LANGUAGE' : 'offensive_analysis'\r\n",
        "                       }\r\n",
        "# class dictionary, which containes all the classes specific to a given task as values                       \r\n",
        "class_dict = {'SENTIMENT_ANALYSIS' :['Negative', 'Neutral', 'Positive'],\r\n",
        "              'HATE_ANALYSIS' : ['Not-hate', 'hate'],\r\n",
        "              'OFFENSIVE_LANGUAGE' : ['Not-offensive', 'offensive']}\r\n",
        "# Flag to perform EDA for all the three tasks in loop, by default it is fault\r\n",
        "# as we do the visualization separately\r\n",
        "if TRAIN_ALL:\r\n",
        "  for classification_task, task in classification_task_dict.items():\r\n",
        "    print('=========================================')\r\n",
        "    print('CLASSIFICATION TASK: {}'.format(classification_task))\r\n",
        "    print('=========================================')\r\n",
        "    if classification_task == 'SENTIMENT_ANALYSIS':\r\n",
        "      train_df, val_df, test_df = prepare_dataset(SENTIMENT_TRAIN_TEXT, SENTIMENT_TRAIN_LABEL,\r\n",
        "                          SENTIMENT_VALIDATION_TEXT, SENTIMENT_VALIDATION_LABEL,\r\n",
        "                          SENTIMENT_TEST_TEXT, SENTIMENT_TEST_LABEL, classification_task_dict['SENTIMENT_ANALYSIS'],\r\n",
        "                          EDA=True)\r\n",
        "\r\n",
        "    if classification_task == 'HATE_ANALYSIS':\r\n",
        "      train_df, val_df, test_df = prepare_dataset(HATE_TRAIN_TEXT, HATE_TRAIN_LABEL,\r\n",
        "                          HATE_VALIDATION_TEXT, HATE_VALIDATION_LABEL,\r\n",
        "                          HATE_TEST_TEXT, HATE_TEST_LABEL, classification_task_dict['HATE_ANALYSIS'],\r\n",
        "                          EDA=True)\r\n",
        "\r\n",
        "    if classification_task == 'OFFENSIVE_LANGUAGE':\r\n",
        "      train_df, val_df, test_df = prepare_dataset(OFFENSE_TRAIN_TEXT, OFFENSE_TRAIN_LABEL,\r\n",
        "                          OFFENSE_VALIDATION_TEXT, OFFENSE_VALIDATION_LABEL,\r\n",
        "                          OFFENSE_TEST_TEXT, OFFENSE_TEST_LABEL, classification_task_dict['OFFENSIVE_LANGUAGE'],\r\n",
        "                          EDA=True)\r\n",
        "    perform_eda(train_df)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIN-lj-EwpyE"
      },
      "source": [
        "# Sentiment Analysis Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W52xiCXUE3IJ"
      },
      "source": [
        "classification_task = 'SENTIMENT_ANALYSIS'\r\n",
        "train_df, val_df, test_df = prepare_dataset(SENTIMENT_TRAIN_TEXT, SENTIMENT_TRAIN_LABEL,\r\n",
        "                    SENTIMENT_VALIDATION_TEXT, SENTIMENT_VALIDATION_LABEL,\r\n",
        "                    SENTIMENT_TEST_TEXT, SENTIMENT_TEST_LABEL, classification_task_dict['SENTIMENT_ANALYSIS'],\r\n",
        "                    EDA=True)\r\n",
        "perform_eda(train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIIT4blawunc"
      },
      "source": [
        "# Hate Word Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-tHpLO7C-tv"
      },
      "source": [
        "classification_task = 'HATE_ANALYSIS'\r\n",
        "train_df, val_df, test_df = prepare_dataset(HATE_TRAIN_TEXT, HATE_TRAIN_LABEL,\r\n",
        "                    HATE_VALIDATION_TEXT, HATE_VALIDATION_LABEL,\r\n",
        "                    HATE_TEST_TEXT, HATE_TEST_LABEL, classification_task_dict['HATE_ANALYSIS'],\r\n",
        "                    EDA=True)\r\n",
        "perform_eda(train_df)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VXYsuHcw0vj"
      },
      "source": [
        "# Offensive Language Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6EpCKlxEuKC"
      },
      "source": [
        "classification_task = 'OFFENSIVE_LANGUAGE'\r\n",
        "train_df, val_df, test_df = prepare_dataset(OFFENSE_TRAIN_TEXT, OFFENSE_TRAIN_LABEL,\r\n",
        "                    OFFENSE_VALIDATION_TEXT, OFFENSE_VALIDATION_LABEL,\r\n",
        "                    OFFENSE_TEST_TEXT, OFFENSE_TEST_LABEL, classification_task_dict['OFFENSIVE_LANGUAGE'],\r\n",
        "                    EDA=True)\r\n",
        "perform_eda(train_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}