{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset_eda.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VXEu82YYZjpR"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwkhan/CE888/blob/main/assignment1/dataset_eda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXEu82YYZjpR"
      },
      "source": [
        "# Importing Necesseties "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP86rh1kb7T7",
        "outputId": "fce6425d-b768-467a-9096-faad65dee2bf"
      },
      "source": [
        "!pip install --upgrade plotly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: plotly in /usr/local/lib/python3.6/dist-packages (4.14.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from plotly) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly) (1.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8oc23Nm0cL-",
        "outputId": "74681613-ecd1-4ac3-bd28-8f75e5eb3fce"
      },
      "source": [
        "  import nltk\r\n",
        "  nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agFFVKotZjpS"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np \n",
        "import random\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from plotly import graph_objs as go\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "from collections import Counter\n",
        "\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import nltk\n",
        "import spacy\n",
        "import random\n",
        "from spacy.util import compounding\n",
        "from spacy.util import minibatch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ9aYIWCZjpW"
      },
      "source": [
        "# Reading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HczXkD6TB_E"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "# Marry nltk and ScikitLearn\r\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\r\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\r\n",
        "import pickle\r\n",
        "from nltk import word_tokenize\r\n",
        "import nltk\r\n",
        "import random\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.classify import ClassifierI\r\n",
        "from statistics import mode\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import seaborn as sns\r\n",
        "import requests\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyRv_nGhQ1lh"
      },
      "source": [
        "############################ SENTIMENT ANALYSIS #################################################\r\n",
        "SENTIMENT_TRAIN_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_text.txt'\r\n",
        "SENTIMENT_VALIDATION_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_text.txt'\r\n",
        "SENTIMENT_TEST_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/test_text.txt'\r\n",
        "\r\n",
        "SENTIMENT_TRAIN_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_labels.txt'\r\n",
        "SENTIMENT_VALIDATION_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_labels.txt'\r\n",
        "SENTIMENT_TEST_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/test_labels.txt'\r\n",
        "\r\n",
        "############################ HATE #################################################\r\n",
        "HATE_TRAIN_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_text.txt'\r\n",
        "HATE_VALIDATION_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_text.txt'\r\n",
        "HATE_TEST_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_text.txt'\r\n",
        "\r\n",
        "HATE_TRAIN_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_labels.txt'\r\n",
        "HATE_VALIDATION_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_labels.txt'\r\n",
        "HATE_TEST_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_labels.txt'\r\n",
        "\r\n",
        "############################ OFFENSIVE LANGUAGE#################################################\r\n",
        "OFFENSE_TRAIN_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_text.txt'\r\n",
        "OFFENSE_VALIDATION_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_text.txt'\r\n",
        "OFFENSE_TEST_TEXT = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_text.txt'\r\n",
        "\r\n",
        "OFFENSE_TRAIN_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_labels.txt'\r\n",
        "OFFENSE_VALIDATION_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_labels.txt'\r\n",
        "OFFENSE_TEST_LABEL = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_labels.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSBd5mXFQ2n8"
      },
      "source": [
        "def preprocess(df, EDA=False): \r\n",
        "    lemmatizer  = WordNetLemmatizer()\r\n",
        "    ignore_words = ['user', 'st'] \r\n",
        "    df['processed_tweets'] = df['tweet'].replace('[^a-zA-Z]',' ', regex=True,\r\n",
        "                                                  inplace=False)\r\n",
        "    \r\n",
        "    df['processed_tweets'] = df['processed_tweets'].apply(lambda x: [w.lower() for w in x.split()])\r\n",
        "\r\n",
        "    if not EDA:\r\n",
        "      df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ([word for word in tweet if not word in stopwords.words(\"english\")]))\r\n",
        "      df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ([lemmatizer.lemmatize(word) for word in tweet]))\r\n",
        "    else:\r\n",
        "      df['processed_tweets'] = df['processed_tweets'].apply(lambda tweet: ' '.join([word for word in tweet if len(word)>2]))\r\n",
        "      df['processed_tweets'] = df['processed_tweets'].apply(lambda x: ' '.join([word for word in x.split() if not word in ignore_words]))\r\n",
        "    \r\n",
        "    df[\"sentence_length\"] = df.tweet.apply(lambda x: len(str(x).split()))\r\n",
        "    return df\r\n",
        "\r\n",
        "\r\n",
        "# Wrapper to convert text data to pandas Dataframe\r\n",
        "def txt_to_df(data, label, classification_task):\r\n",
        "    tweet = []\r\n",
        "    sentiments = []\r\n",
        "    for sentence in data.split('\\n'):\r\n",
        "        tweet.append(sentence)\r\n",
        "    for sentiment in label.split('\\n'):\r\n",
        "        try:\r\n",
        "            sentiments.append(int(sentiment))\r\n",
        "        except ValueError:\r\n",
        "            pass\r\n",
        "    df= pd.DataFrame(tweet[:-1], columns=['tweet'])\r\n",
        "    df['label'] = sentiments\r\n",
        "    if classification_task == 'Sentiment_analysis':\r\n",
        "      df['sentiment'] = df.label.apply(lambda x: 'Negative'if x==0 else ('Neutral' if x==1 else 'Positive'))\r\n",
        "    if classification_task == 'hate_analysis':\r\n",
        "      df['sentiment'] = df.label.apply(lambda x: 'Not-hate'if x==0 else 'hate')\r\n",
        "    if classification_task == 'offensive_analysis':\r\n",
        "      df['sentiment'] = df.label.apply(lambda x: 'Not-offensive'if x==0 else 'offensive')\r\n",
        "    return df\r\n",
        "\r\n",
        "\r\n",
        "def prepare_dataset(TRAIN_TEXT, TRAIN_LABEL, VAL_TEXT, VAL_LABEL, TEST_TEXT, TEST_LABEL, classification_task, EDA=False):\r\n",
        "  # Reading Train, Vvalidation & Test data from tweeteval Github Repo.\r\n",
        "  train_tweets_txt = requests.get(TRAIN_TEXT).text\r\n",
        "  train_labels_txt = requests.get(TRAIN_LABEL).text\r\n",
        "\r\n",
        "  val_tweets_txt = requests.get(VAL_TEXT).text\r\n",
        "  val_labels_txt = requests.get(VAL_LABEL).text\r\n",
        "\r\n",
        "  test_tweets_txt = requests.get(TEST_TEXT).text\r\n",
        "  test_labels_txt = requests.get(TEST_LABEL).text\r\n",
        "\r\n",
        "  # Converting text data to pandas Dataframe\r\n",
        "  train_df = txt_to_df(train_tweets_txt, train_labels_txt, classification_task)\r\n",
        "  val_df = txt_to_df(val_tweets_txt, val_labels_txt, classification_task)\r\n",
        "  test_df = txt_to_df(test_tweets_txt, test_labels_txt, classification_task)\r\n",
        "\r\n",
        "  train_df = preprocess(train_df, EDA)\r\n",
        "  val_df = preprocess(val_df, EDA)\r\n",
        "  test_df = preprocess(test_df, EDA)  \r\n",
        "\r\n",
        "  return train_df, val_df, test_df\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnGgn6bxZjpg"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taqEU7VdZjpk"
      },
      "source": [
        "Lets look at the distribution of tweets in the train set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrXH7WvsZjpn"
      },
      "source": [
        "Let's draw a Funnel-Chart for better visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaueIE7iZjpn"
      },
      "source": [
        "def plot_class_distribution(data):\n",
        "  eda_df = data.groupby('sentiment').count()['tweet'].reset_index().sort_values(by='tweet',ascending=False)\n",
        "  fig = go.Figure(go.Funnelarea(\n",
        "      text =eda_df.sentiment,\n",
        "      values = eda_df.tweet,\n",
        "      title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"},\n",
        "      ))\n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGzfUE7pZjp0"
      },
      "source": [
        "## Most Frequent words in Processed Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgaJqrxvZjp2"
      },
      "source": [
        "def remove_stopword(x):\n",
        "    return [y for y in x if y not in stopwords.words('english')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qKWuCXrcnaQ"
      },
      "source": [
        "def Get_word_frequency(data):\r\n",
        "  frequent_words = Counter([item for sublist in data['words'] for item in sublist])\r\n",
        "  frequent_20_words = pd.DataFrame(frequent_words.most_common(20))\r\n",
        "  frequent_20_words.columns = ['Common_words','count']\r\n",
        "  return frequent_20_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi3P_tnjZjp1"
      },
      "source": [
        "def plot_word_frequency(frequent_20_words, sentiment=''):\n",
        "  fig = px.bar(frequent_20_words, x=\"count\", y=\"Common_words\", title=f'Most Frequent {sentiment} Words', orientation='h', width=800, height=500, color='Common_words')\n",
        "  fig.show()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI4eBLA7Zjp2"
      },
      "source": [
        "def plot_tree_map(frequent_20_words, sentiment=''):\n",
        "  fig = px.treemap(frequent_20_words, path=['Common_words'], values='count',title=f'Tree of Most Frequent {sentiment} Words', width=1000, height=500, color='Common_words')\n",
        "  fig.show()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLfEyXbrcW54"
      },
      "source": [
        "def train_data_eda(data):\n",
        "  plot_class_distribution(data)\n",
        "  data['words'] = data['processed_tweets'].apply(lambda x:str(x).split())\n",
        "  data['words'] = data['words'].apply(lambda x:remove_stopword(x))\n",
        "  tweets_word_frequency = Get_word_frequency(data)\n",
        "  tweets_word_frequency.style.background_gradient(cmap='cool_r')\n",
        "  \n",
        "  plot_word_frequency(tweets_word_frequency)\n",
        "  plot_tree_map(tweets_word_frequency)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv4gf_S7Zjp5"
      },
      "source": [
        "# Most common words Sentiments Wise\n",
        "\n",
        "Let's look at the most common words in different sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV9buiPQh35W"
      },
      "source": [
        "def plot_words_class_basis(data):\r\n",
        "  # Create list of words specific to Each class\r\n",
        "  class_df_list = [] # list of all the classes DataFrame e.g. negative_df, posivtive_df, neutral_df\r\n",
        "  class_specific_word_frequency = []\r\n",
        "\r\n",
        "  for i in range(len(class_dict[classification_task])):\r\n",
        "    class_df_list.append(data[data['sentiment']==class_dict[classification_task][i]])\r\n",
        "\r\n",
        "  # Get frequency of each word for all the classes\r\n",
        "  for i in range(len(class_dict[classification_task])):\r\n",
        "      class_specific_word_frequency.append(Get_word_frequency(class_df_list[i]))\r\n",
        "\r\n",
        "  for i in range(len(class_dict[classification_task])):\r\n",
        "    plot_word_frequency(class_specific_word_frequency[i], class_dict[classification_task][i])\r\n",
        "\r\n",
        "  for i in range(len(class_dict[classification_task])):\r\n",
        "    plot_tree_map(class_specific_word_frequency[i], class_dict[classification_task][i])\r\n",
        "  return class_df_list\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRDi-qjQZjp_"
      },
      "source": [
        "* We can see words like get,go,dont,got,u,cant,lol,like are common in all three segments . That's interesting because words like dont and cant are more of negative nature and words like lol are more of positive nature.Does this mean our data is incorrectly labelled , we will have more insights on this after N-gram analysis\n",
        "* It will be interesting to see the word unique to different sentiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpEaDsl8ZjqF"
      },
      "source": [
        "## It's Time For WordClouds\n",
        "\n",
        "We will be building wordclouds in the following order:\n",
        "\n",
        "* WordCloud of Neutral Tweets\n",
        "* WordCloud of Positive Tweets\n",
        "* WordCloud of Negative Tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUWRs035oxpd"
      },
      "source": [
        "def flatten_list(l):\r\n",
        "    return [x for y in l for x in y]\r\n",
        "\r\n",
        "# color coding our wordclouds \r\n",
        "def red_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\r\n",
        "    return f\"hsl(0, 100%, {random.randint(25, 75)}%)\" \r\n",
        "\r\n",
        "def green_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\r\n",
        "    return f\"hsl({random.randint(90, 150)}, 100%, 30%)\" \r\n",
        "\r\n",
        "def yellow_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\r\n",
        "    return f\"hsl(42, 100%, {random.randint(25, 50)}%)\" \r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "def generate_word_clouds(class_df_list_flatten):\r\n",
        "    # Display the generated image:\r\n",
        "\r\n",
        "    color_funct_list = [red_color_func, green_color_func, yellow_color_func ] \r\n",
        "\r\n",
        "    fig, axes = plt.subplots(1,len(class_df_list_flatten), figsize=(20,10))\r\n",
        "    wordcloud_class_list = []\r\n",
        "    for i in range(len(class_df_list_flatten)):\r\n",
        "\r\n",
        "      wordcloud_class_list.append(WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(class_df_list_flatten[i])))\r\n",
        "      axes[i].imshow(wordcloud_class_list[i].recolor(color_func=color_funct_list[i], random_state=3), interpolation='bilinear')\r\n",
        "      axes[i].set_title(class_dict[classification_task][i])\r\n",
        "      axes[i].axis(\"off\")\r\n",
        "\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZptLmiq0zIM"
      },
      "source": [
        "def visualize_word_clouds(class_df_list):\r\n",
        "  class_df_list_flatten = []\r\n",
        "  for i in range(len(class_dict[classification_task])):\r\n",
        "    class_df_list_flatten.append(flatten_list(class_df_list[i]['words']))\r\n",
        "  generate_word_clouds(class_df_list_flatten)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvIhyRSvziDD"
      },
      "source": [
        "# **Vectorization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2ph1HEOZjqS"
      },
      "source": [
        "from gensim.models import word2vec\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "\r\n",
        "def tsne_plot(data):\r\n",
        "    \"Creates and TSNE model and plots it\"\r\n",
        "    labels = []\r\n",
        "    tokens = []\r\n",
        "    model = word2vec.Word2Vec(data.words, size=300, window=20, min_count=50, workers=4)\r\n",
        "    for word in model.wv.vocab:\r\n",
        "        tokens.append(model[word])\r\n",
        "        labels.append(word)\r\n",
        "    \r\n",
        "    tsne_model = TSNE(perplexity=40, n_components=3, init='pca', n_iter=2500, random_state=23)\r\n",
        "    new_values = tsne_model.fit_transform(tokens)\r\n",
        "\r\n",
        "    x = []\r\n",
        "    y = []\r\n",
        "    for value in new_values:\r\n",
        "        x.append(value[0])\r\n",
        "        y.append(value[1])\r\n",
        "        \r\n",
        "    plt.figure(figsize=(20, 16)) \r\n",
        "    for i in range(len(x)):\r\n",
        "        plt.scatter(x[i],y[i])\r\n",
        "        plt.annotate(labels[i],\r\n",
        "                     xy=(x[i], y[i]),\r\n",
        "                     xytext=(5, 2),\r\n",
        "                     textcoords='offset points',\r\n",
        "                     ha='right',\r\n",
        "                     va='bottom')\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwrS6embxaxQ"
      },
      "source": [
        "# to split 30% to visualize later using PCA\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\r\n",
        "import matplotlib\r\n",
        "import matplotlib.patches as mp\r\n",
        "\r\n",
        "def visualize_class_distribution_3d(data):\r\n",
        "  sns.set()\r\n",
        "  count_vect = CountVectorizer(ngram_range=(1,3), lowercase=False)\r\n",
        "  _, X_, _, y_, = train_test_split(data.processed_tweets.tolist(), data.sentiment.tolist(), test_size = 0.9)\r\n",
        "  X_ = count_vect.fit_transform(X_)\r\n",
        "  y_ = count_vect.transform(y_)\r\n",
        "  unique_label = class_dict[classification_task]\r\n",
        "\r\n",
        "\r\n",
        "  data_visual = TruncatedSVD(n_components = 3).fit_transform(X_)\r\n",
        "  plt.rcParams[\"figure.figsize\"] = [21, 10]\r\n",
        "  ax = plt.subplot(111, projection='3d')\r\n",
        "  colors = ['red', 'blue', 'green']\r\n",
        "  color_patch = []\r\n",
        "  for i in range(len(class_dict[classification_task])):\r\n",
        "    color_patch.append(mp.Patch(color=colors[i],label=class_dict[classification_task][i]))\r\n",
        "  current_palette = sns.color_palette()\r\n",
        "  x = data_visual[:,0]\r\n",
        "  y = data_visual[:, 1]\r\n",
        "  z =  data_visual[:, 2]\r\n",
        "  for no, _ in enumerate(np.unique(y_)):\r\n",
        "      ax.scatter3D(x, y, z, c=x, label = unique_label[no], alpha = 0.5,\r\n",
        "                  cmap=matplotlib.colors.ListedColormap(colors))\r\n",
        "      \r\n",
        "      # plt.legend()\r\n",
        "      \r\n",
        "  box = ax.get_position()\r\n",
        "  ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 1])\r\n",
        "  ax.legend(handles=color_patch, loc = 'upper center', fancybox = True, shadow = True, ncol = 3)\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8B6j1J5_tx5"
      },
      "source": [
        "def perform_eda(training_data):\r\n",
        "  data = train_data_eda(training_data)\r\n",
        "  class_df_list = plot_words_class_basis(data)\r\n",
        "  visualize_word_clouds(class_df_list)\r\n",
        "  tsne_plot(training_data)\r\n",
        "  visualize_class_distribution_3d(training_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD7Sz4mmzOcu"
      },
      "source": [
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "def initiate_eda():\r\n",
        "  classification_task_dict = {'SENTIMENT_ANALYSIS' : 'Sentiment_analysis',\r\n",
        "                        'HATE_ANALYSIS' : 'hate_analysis',\r\n",
        "                        'OFFENSIVE_LANGUAGE' : 'offensive_analysis'\r\n",
        "                        }\r\n",
        "  class_dict = {'SENTIMENT_ANALYSIS' :['Negative', 'Neutral', 'Positive'],\r\n",
        "                'HATE_ANALYSIS' : ['Not-hate', 'hate'],\r\n",
        "                'OFFENSIVE_LANGUAGE' : ['Not-offensive', 'offensive']}\r\n",
        "\r\n",
        "  for classification_task, task in classification_task_dict.items():\r\n",
        "    print('=========================================')\r\n",
        "    print('CLASSIFICATION TASK: {}'.format(classification_task))\r\n",
        "    print('=========================================')\r\n",
        "    if classification_task == 'SENTIMENT_ANALYSIS':\r\n",
        "      continue\r\n",
        "      train_df, val_df, test_df = prepare_dataset(SENTIMENT_TRAIN_TEXT, SENTIMENT_TRAIN_LABEL,\r\n",
        "                          SENTIMENT_VALIDATION_TEXT, SENTIMENT_VALIDATION_LABEL,\r\n",
        "                          SENTIMENT_TEST_TEXT, SENTIMENT_TEST_LABEL, classification_task_dict['SENTIMENT_ANALYSIS'],\r\n",
        "                          EDA=True)\r\n",
        "\r\n",
        "    if classification_task == 'HATE_ANALYSIS':\r\n",
        "      train_df, val_df, test_df = prepare_dataset(HATE_TRAIN_TEXT, HATE_TRAIN_LABEL,\r\n",
        "                          HATE_VALIDATION_TEXT, HATE_VALIDATION_LABEL,\r\n",
        "                          HATE_TEST_TEXT, HATE_TEST_LABEL, classification_task_dict['HATE_ANALYSIS'],\r\n",
        "                          EDA=True)\r\n",
        "\r\n",
        "    if classification_task == 'OFFENSIVE_LANGUAGE':\r\n",
        "      train_df, val_df, test_df = prepare_dataset(OFFENSE_TRAIN_TEXT, OFFENSE_TRAIN_LABEL,\r\n",
        "                          OFFENSE_VALIDATION_TEXT, OFFENSE_VALIDATION_LABEL,\r\n",
        "                          OFFENSE_TEST_TEXT, OFFENSE_TEST_LABEL, classification_task_dict['OFFENSIVE_LANGUAGE'],\r\n",
        "                          EDA=True)\r\n",
        "    perform_eda(train_df)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W52xiCXUE3IJ"
      },
      "source": [
        "# train_df, val_df, test_df = prepare_dataset(SENTIMENT_TRAIN_TEXT, SENTIMENT_TRAIN_LABEL,\r\n",
        "#                     SENTIMENT_VALIDATION_TEXT, SENTIMENT_VALIDATION_LABEL,\r\n",
        "#                     SENTIMENT_TEST_TEXT, SENTIMENT_TEST_LABEL, classification_task_dict['SENTIMENT_ANALYSIS'],\r\n",
        "#                     EDA=True)\r\n",
        "# perform_eda(train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-tHpLO7C-tv"
      },
      "source": [
        "# train_df, val_df, test_df = prepare_dataset(HATE_TRAIN_TEXT, HATE_TRAIN_LABEL,\r\n",
        "#                     HATE_VALIDATION_TEXT, HATE_VALIDATION_LABEL,\r\n",
        "#                     HATE_TEST_TEXT, HATE_TEST_LABEL, classification_task_dict['HATE_ANALYSIS'],\r\n",
        "#                     EDA=True)\r\n",
        "# perform_eda(train_df)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6EpCKlxEuKC"
      },
      "source": [
        "# train_df_ol, val_df, test_df = prepare_dataset(OFFENSE_TRAIN_TEXT, OFFENSE_TRAIN_LABEL,\r\n",
        "#                     OFFENSE_VALIDATION_TEXT, OFFENSE_VALIDATION_LABEL,\r\n",
        "#                     OFFENSE_TEST_TEXT, OFFENSE_TEST_LABEL, classification_task_dict['OFFENSIVE_LANGUAGE'],\r\n",
        "#                     EDA=True)\r\n",
        "# perform_eda(train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJZXXqFIGsVJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}